{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "02215aa5-5538-4c38-8b02-f811be21198b",
    "_uuid": "ff0d9a7703013e9c37049e9da7915063ab03c938"
   },
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input, Dense, Conv1D, GlobalMaxPool1D, GlobalAvgPool1D, Concatenate,\\\n",
    "Embedding, Lambda, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import optimizers, activations\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9cce7bba48f784f69b62fb389f1be2c3da12da10",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES_NAME = 75000\n",
    "MAX_FEATURES_DESC = 125000\n",
    "MAX_TEXT_NAME = 40000\n",
    "MAX_TEXT_DESC = 80000\n",
    "MAX_SEQ_NAME = 20\n",
    "MAX_SEQ_DESC = 64\n",
    "MIN_DF_NAME = 2\n",
    "MIN_DF_DESC = 80\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 512 * 8\n",
    "VERBOSE = 1\n",
    "VALID_SPLIT = 0.0\n",
    "\n",
    "models = []\n",
    "pool = ThreadPool(processes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For features\n",
    "def reformat_name(text):\n",
    "    REPLACE = [\n",
    "        (re.compile(r\"[^a-z0-9\\.\\\"]\"), r\" \"),\n",
    "        (re.compile(r\"([a-z]+)\"), r\" \\1 \"),\n",
    "    ]\n",
    "    \n",
    "    text = text.lower()\n",
    "    text_2 = \" \" + text\n",
    "    for regexp, substitution in REPLACE:\n",
    "        text_2 = regexp.sub(substitution, text_2)\n",
    "        \n",
    "    return text, text_2\n",
    "\n",
    "def reformat_desc(text):\n",
    "    return reformat_name(text)[1]\n",
    "\n",
    "def split_cat(text):\n",
    "    try:\n",
    "        return text.split(\"/\")\n",
    "    except:\n",
    "        return (\"Other\", \"Other\", \"Other\")\n",
    "\n",
    "def handel_df_inplace(df):\n",
    "    df['name_1'] = df['name'].str.get(0).fillna('missing')\n",
    "    df['name_2'] = df['name'].str.get(1).fillna('missing')\n",
    "    df.drop('name', axis=1, inplace=True)\n",
    "    df['item_condition_id'] = df['item_condition_id'].cat.add_categories(['missing']).fillna('missing')\n",
    "    df['gencat_name'] = df['category_name'].str.get(0).replace('', 'Other').astype('category')\n",
    "    df['subcat1_name'] = df['category_name'].str.get(1).fillna('Other').astype('category')\n",
    "    df['subcat2_name'] = df['category_name'].str.get(2).fillna('Other').astype('category')\n",
    "    df.drop('category_name', axis=1, inplace=True)\n",
    "    df['brand_name'] = df['brand_name'].fillna('missing').astype('category')\n",
    "    df['shipping'] = df['shipping'].cat.add_categories(['missing']).fillna('missing')\n",
    "    df['item_description'] = df['item_description'].fillna('missing')\n",
    "    df['item_description'] = df.item_description.apply(lambda x: 'missing' if x=='no description yet' else x)\n",
    "    df['desc_len'] = df.item_description.apply(lambda x: 0 if x=='missing' else len(x.split())).astype('category')\n",
    "\n",
    "    return None\n",
    "\n",
    "# For models\n",
    "def rmsle(y_true, y_pred):\n",
    "    return np.mean((np.log1p(y_pred) - np.log1p(y_true)) ** 2) ** 0.5\n",
    "\n",
    "def pred_models(models, X, batch_size=2048, target_to_price_func=np.expm1):\n",
    "    val_preds = [model.predict(X, batch_size=batch_size) for model in models]\n",
    "    val_preds = sum(val_preds)/len(val_preds)\n",
    "    val_preds = target_to_price_func(val_preds)\n",
    "    \n",
    "    return val_preds\n",
    "\n",
    "def eval_models(models, X_valid, y_true, batch_size=2048, target_to_price_func=np.expm1):\n",
    "    val_preds = pred_models(models, X_valid, batch_size=batch_size, target_to_price_func=target_to_price_func)\n",
    "    \n",
    "    return rmsle(y_true, val_preds[:, 0])\n",
    "\n",
    "def exp_decay(init, fin, steps):\n",
    "    return (init/fin)**(1/(steps-1)) - 1\n",
    "\n",
    "def get_model_1(output_func=None):\n",
    "    vec = Input(shape=[X_train[\"vec\"].shape[1]], name=\"vec\", sparse=True)\n",
    "    \n",
    "    h_layer = Dropout(0.1) (Dense(128, activation=lambda x: activations.elu(x, alpha=.5)) (vec))\n",
    "    h_layer = Dense(64, activation=activations.elu(x, alpha=.5)) (h_layer)\n",
    "    h_layer = Dense(64, activation=activations.linear) (h_layer)\n",
    "    \n",
    "    if output_func is None:\n",
    "        output = Lambda(lambda x: K.mean(K.tanh(x), axis=1, keepdims=True)*4+4) (h_layer)\n",
    "    else:\n",
    "        output = Lambda(output_func) (h_layer)\n",
    "        \n",
    "    model = Model([vec], output)\n",
    "    model.compile(loss=\"mse\",  optimizer=optimizers.adam(beta_1=0.9, beta_2=0.9))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_model_2(output_func=None):\n",
    "    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n",
    "    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n",
    "    sub_vec = Input(shape=[X_train[\"sub_vec\"].shape[1]], name=\"vec\", sparse=True)\n",
    "    \n",
    "    name_layer = Embedding(MAX_TEXT_NAME, 96) (name)\n",
    "    name_layer = GlobalMaxPool1D() (name_layer)\n",
    "    \n",
    "    item_desc_layer = Embedding(MAX_TEXT_DESC, 64) (item_desc)\n",
    "    item_desc_layer = Conv1D(3, 1) (item_desc_layer)\n",
    "    item_desc_layer = GlobalAvgPool1D() (item_desc_layer)\n",
    "    \n",
    "    vec_layer = Dropout(.1) (Dense(32, activation=lambda x: activations.elu(x, alpha=.5)) (sub_vec))\n",
    "    \n",
    "    h_layer = Concatenate() ([\n",
    "        name_layer,\n",
    "        item_desc_layer,\n",
    "        vec_layer\n",
    "    ])\n",
    "    \n",
    "    h_layer = Dense(64, activation=lambda x: activations.elu(x, alpha=.5)) (h_layer)\n",
    "    h_layer = Dense(64) (h_layer)\n",
    "    \n",
    "    if output_func is None:\n",
    "        output = Lambda(lambda x: K.mean(K.tanh(x), axis=1, keepdims=True)*4+4) (h_layer)\n",
    "    else:\n",
    "        output = Lambda(output_func) (h_layer)\n",
    "        \n",
    "    model = Model([name, item_desc, vec], output)\n",
    "    model.compile(loss=\"mse\",  optimizer=optimizers.adam(beta_1=0.9, beta_2=0.9))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field, dtype=None):\n",
    "        self.field = field\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataframe):\n",
    "        if self.dtype == 'category':\n",
    "            return dataframe[self.field].cat.codes[:, None]\n",
    "        else:\n",
    "            return dataframe[self.field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "713a1b16b05a170fa553744ff4e2b3ba018cff01"
   },
   "outputs": [],
   "source": [
    "def thread():\n",
    "    train = pd.read_table('../input/train.tsv', engine='c', \n",
    "                          dtype={'item_condition_id': 'category', 'shipping': 'category'},\n",
    "                          converters={'category_name': split_cat, 'name': reformat_name,\n",
    "                                      'item_description': reformat_desc})\n",
    "    test_ = pd.read_table('../input/test.tsv', engine='c', \n",
    "                          dtype={'item_condition_id': 'category', 'shipping': 'category'},\n",
    "                          converters={'category_name': split_cat, 'name': reformat_name,\n",
    "                                      'item_description': reformat_desc})\n",
    "    \n",
    "    train_df, dev_df = train_test_split(train, random_state=123, train_size=0.99)\n",
    "    train_df = train_df.loc[train_df.price > 0]\n",
    "    train_df['brand_name'] = train_df['brand_name'].fillna('missing').astype('category')\n",
    "    test_['brand_name'] = test_.brand_name.apply(lambda x: x if x in train_df.brand_name.cat.categories else 'missing').astype('category')\n",
    "    n_trains, n_devs = train_df.shape[0], dev_df.shape[0]\n",
    "    merge = pd.concat([train_df, dev_df, test_])\n",
    "    handel_df_inplace(merge)\n",
    "    \n",
    "    return merge, n_trains, n_devs\n",
    "\n",
    "merge, n_trains, n_devs = pool.apply_async(thread).get()\n",
    "\n",
    "def thread():\n",
    "    vec = FeatureUnion([\n",
    "        ('name_1', Pipeline([\n",
    "            ('selector', ItemSelector(field='name_1')),\n",
    "            ('cv', CountVectorizer(max_features=MAX_FEATURES_NAME)),\n",
    "        ])),\n",
    "        ('item_description', Pipeline([\n",
    "            ('selector', ItemSelector(field='item_description')),\n",
    "            ('cv', CountVectorizer(max_features=MAX_FEATURES_DESC)),\n",
    "            ('tfidf', TfidfTransformer(max_features=MAX_FEATURES_DESC))\n",
    "        ])),\n",
    "        ('item_condition_id', Pipeline([\n",
    "            ('selector', ItemSelector(field='item_condition_id', dtype='category')),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('shipping', Pipeline([\n",
    "            ('selector', ItemSelector(field='shipping', dtype='category')),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('gencat_name', Pipeline([\n",
    "            ('selector', ItemSelector(field='gencat_name', dtype='category')),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('subcat1_name', Pipeline([\n",
    "            ('selector', ItemSelector(field='subcat1_name', dtype='category')),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('subcat2_name', Pipeline([\n",
    "            ('selector', ItemSelector(field='subcat2_name', dtype='category')),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('brand_name', Pipeline([\n",
    "            ('selector', ItemSelector(field='brand_name', dtype='category')),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('desc_len', Pipeline([\n",
    "            ('selector', ItemSelector(field='desc_len', dtype='category')),\n",
    "            ('ohe', OneHotEncoder(handle_unknown='ignore', dtype=np.int8))\n",
    "        ]))\n",
    "    ], n_jobs=-1).fit_transform(merge)\n",
    "    \n",
    "    train_vec = vec[:n_trains]\n",
    "    dev_vec = vec[n_trains:n_trains+n_devs]\n",
    "    test_vec = vec[n_trains+n_devs:]\n",
    "    \n",
    "    return train_vec, dev_vec, test_vec\n",
    "\n",
    "train_vec, dev_vec, test_vec = pool.apply_async(thread).get()\n",
    "train_sub_vec = train_vec[MAX_FEATURES_NAME+MAX_FEATURES_DESC*2:]\n",
    "dev_sub_vec = dev_vec[MAX_FEATURES_NAME+MAX_FEATURES_DESC*2:]\n",
    "test_sub_vec = test_vec[MAX_FEATURES_NAME+MAX_FEATURES_DESC*2:]\n",
    "\n",
    "dtrain = pd.DataFrame(index=merge[:n_trains].index)\n",
    "ddev = pd.DataFrame(index=merge[n_trains:n_trains+n_devs].index)\n",
    "dtest = pd.DataFrame(index=merge[n_trains+n_devs:].index)\n",
    "dtrain['target'] = np.log1p(merge[:n_trains].price)\n",
    "ddev['price'] = merge[n_trains:n_trains+n_devs].price\n",
    "\n",
    "def thread():\n",
    "    tok_raw_name = Tokenizer(num_words=MAX_TEXT_NAME, filters='', lower=False)\n",
    "    tok_raw_name.fit_on_texts(merge.name_2)\n",
    "    tok_raw_desc = Tokenizer(num_words=MAX_TEXT_DESC, filters='', lower=False)\n",
    "    tok_raw_desc.fit_on_texts(merge.item_description)\n",
    "\n",
    "    dtrain[\"seq_name\"] = tok_raw_name.texts_to_sequences(merge[:n_trains].name_2)\n",
    "    dtrain[\"seq_item_description\"] = tok_raw_desc.texts_to_sequences(merge[:n_trains].item_description)\n",
    "    ddev[\"seq_name\"] = tok_raw_name.texts_to_sequences(merge[n_trains:n_trains+n_devs].name_2)\n",
    "    ddev[\"seq_item_description\"] = tok_raw_desc.texts_to_sequences(merge[n_trains:n_trains+n_devs].item_description)\n",
    "    dtest[\"seq_name\"] = tok_raw_name.texts_to_sequences(merge[n_trains+n_devs:].name_2)\n",
    "    dtest[\"seq_item_description\"] = tok_raw_desc.texts_to_sequences(merge[n_trains+n_devs:].item_description)\n",
    "    \n",
    "    return None\n",
    "\n",
    "pool.apply_async(thread).get()\n",
    "\n",
    "X_train = {\n",
    "        'name': pad_sequences(dtrain['seq_name'], maxlen=MAX_SEQ_NAME),\n",
    "        'item_desc': pad_sequences(dtrain['seq_item_description'], maxlen=MAX_SEQ_DESC),\n",
    "        'vec': train_vec,\n",
    "        'sub_vec' train_sub_vec,\n",
    "    }\n",
    "X_dev = {\n",
    "        'name': pad_sequences(ddev['seq_name'], maxlen=MAX_SEQ_NAME),\n",
    "        'item_desc': pad_sequences(ddev['seq_item_description'], maxlen=MAX_SEQ_DESC),\n",
    "        'vec': dev_vec,\n",
    "        'sub_vec' dev_sub_vec,\n",
    "    }\n",
    "X_test = {\n",
    "        'name': pad_sequences(dtest['seq_name'], maxlen=MAX_SEQ_NAME),\n",
    "        'item_desc': pad_sequences(dtest['seq_item_description'], maxlen=MAX_SEQ_DESC),\n",
    "        'vec': test_vec,\n",
    "        'sub_vec' test_sub_vec,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04d30ae58b0805fccc664b4eec3e9a5c19d2892c"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4700d17fb1d25bcca7d7fd933442402ab7f7387a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for lr_i, lr_f, func in [(0.025, 0.005, lambda x: K.mean(K.sigmoid(x), axis=1, keepdims=True)*8),\n",
    "                         (0.025, 0.005, lambda x: K.mean(x/(1+K.abs(x)), axis=1, keepdims=True)*4+4),\n",
    "                         (0.02, 0.002, lambda x: K.mean(x/(1+x**2)**0.5, axis=1, keepdims=True)*4+4),\n",
    "                         (0.017, 0.0017, lambda x: K.mean(K.tanh(x), axis=1, keepdims=True)*4+4)]:\n",
    "    lr_decay = exp_decay(lr_f, lr_f, steps=int(X_train['vec_2'].shape[0]/BATCH_SIZE)*EPOCHS)\n",
    "\n",
    "    model = get_model_1(func)\n",
    "    model.optimizer = optimizers.adam(beta_1=0.9, beta_2=0.9)\n",
    "    model.optimizer.lr = lr_f\n",
    "    model.optimizer.decay = lr_decay\n",
    "\n",
    "    try:\n",
    "        model.fit(X_train, dtrain.target,\n",
    "                  epochs=EPOCHS,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  validation_split=VALID_SPLIT,\n",
    "                  verbose=VERBOSE\n",
    "                 )\n",
    "        print('[RMSLE] {}'.format(eval_models([model], X_dev, ddev.price, batch_size=BATCH_SIZE)))\n",
    "        models.append(model)\n",
    "        print('[ENS RMSLE] {}'.format(eval_models(models, X_dev, ddev.price, batch_size=BATCH_SIZE)))\n",
    "    except:\n",
    "        print('THIS IS STRANGE !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b2bfa8615c17dd2215e2282a954e6fa666dd2e02",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for lr_i, lr_f, func in [(0.009, 0.001, lambda x: K.mean(x/(1+K.abs(x)), axis=1, keepdims=True)*4+4),\n",
    "                         (0.009, 0.001, lambda x: K.mean(x/(1+x**2)**0.5, axis=1, keepdims=True)*4+4),\n",
    "                         (0.009, 0.001, lambda x: K.mean(K.tanh(x), axis=1, keepdims=True)*4+4)]:\n",
    "    lr_decay = exp_decay(lr_i, lr_f, steps=int(X_train['vec_2'].shape[0]/BATCH_SIZE)*EPOCHS)\n",
    "\n",
    "    model = get_model_2(func)\n",
    "    model.optimizer = optimizers.adam(beta_1=0.9, beta_2=0.9)\n",
    "    model.optimizer.lr = lr_i\n",
    "    model.optimizer.decay = lr_decay\n",
    "\n",
    "    try:\n",
    "        model.fit(X_train, dtrain.target,\n",
    "                  epochs=EPOCHS,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  validation_split=VALID_SPLIT,\n",
    "                  verbose=VERBOSE\n",
    "                 )\n",
    "        print('[RMSLE] {}'.format(eval_models([model], X_dev, ddev.price, batch_size=BATCH_SIZE)))\n",
    "        models.append(model)\n",
    "        print('[ENS RMSLE] {}'.format(eval_models(models, X_dev, ddev.price, batch_size=BATCH_SIZE)))\n",
    "    except:\n",
    "        print('THIS IS STRANGE !!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6d25e163b2e24f7e0d30361fc314b6a40e6e2317"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5dc873882018e231b4da4d3c2d22c52b50655d92",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission: pd.DataFrame = merge[n_trains+n_devs:][['test_id']].astype(np.int64)\n",
    "submission['price'] = pred_models(models, X_test)\n",
    "submission.to_csv(\"submission_hope_26.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
